# Neural Network (MLP)
Neural network with fully connected layers, built from scratch.

- Training algorithm: stochastic gradient descent with minibatch (as generalization for both online learning and batch learning)

- Loss function: mean squared error (MSE)

- Activation functions: linear, sigmoid, softplus

- Regularization: L2 (Tichonov) 

- Metrics: accuracy, mean euclidean error (MEE)

- Validation techniques: grid search, k-fold cross validation (with parallelization)

- Utilities: rescaling, 1-of-K encoding, read/write routines for dataset where the project has been tested


This project has been made as part of the course of Machine Learning @ University of Pisa. `MainCup.py` refers to a private contest whose dataset has not been disclosed.  
